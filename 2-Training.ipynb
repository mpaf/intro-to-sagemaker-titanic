{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker XGBoost Algorithm\n",
    "\n",
    "We are going to use the XGBoost algorithm. Documentation can be found here:\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\n",
    "\n",
    "For CSV training, the algorithm assumes that the target variable is in the first column and that the CSV does not have a header record.\n",
    "\n",
    "For a list of hyperparameters, have a look at:\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html\n",
    "\n",
    "A representation of gradient boosted trees algorithm can be found [here](https://www.researchgate.net/figure/A-simple-example-of-visualizing-gradient-boosting_fig5_326379229)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import numpy as np\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# get the URI for the XGBoost container\n",
    "container_image = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='xgboost', version='latest')\n",
    "\n",
    "# build a SageMaker estimator class\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    container_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path='s3://{}/titanic/training'.format(bucket),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# set the hyperparameters\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    max_depth=6,\n",
    "    eta=0.1,\n",
    "    gamma=0,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    "    verbosity=1,\n",
    "    objective='multi:softmax',\n",
    "    num_class=2,\n",
    "    num_round=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset to our S3 bucket\n",
    "input_train = sagemaker_session.upload_data(path='train.csv', key_prefix='titanic')\n",
    "input_val = sagemaker_session.upload_data(path='val.csv', key_prefix='titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run training against the training and val sets created above\n",
    "# Refer to the SageMaker training console\n",
    "\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(input_train, content_type=content_type)\n",
    "validation_input = TrainingInput(input_val, content_type=content_type)\n",
    "\n",
    "xgb_estimator.fit({\n",
    "    'train': train_input,\n",
    "    'validation': validation_input\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save with Spot Instances\n",
    "\n",
    "Let's try to use 'Spot' capacity to train our model. We can also use different hyperparameters to see if we can improve our model. Let's also use logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a SageMaker estimator class\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    container_image,\n",
    "    role,\n",
    "    use_spot_instances=True,\n",
    "    max_run=1200,\n",
    "    max_wait=1800,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path='s3://{}/titanic/training'.format(bucket),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# set the hyperparameters\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    max_depth=6,\n",
    "    eta=0.2,\n",
    "    gamma=2,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.8,\n",
    "    verbosity=1,\n",
    "    objective='binary:logistic',\n",
    "    num_round=15\n",
    ")\n",
    "\n",
    "xgb_estimator.fit({\n",
    "    'train': train_input,\n",
    "    'validation': validation_input\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Script Mode\n",
    "\n",
    "Let's use the same estimator above, but provide our own script `./src/train.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "# build a SageMaker estimator Framework class\n",
    "xgb_estimator = XGBoost(\n",
    "    role=role,\n",
    "    framework_version='1.0-1',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path='s3://{}/titanic/training'.format(bucket),\n",
    "    entry_point=\"./src/train.py\", ## OUR SCRIPT\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        \"num_class\": 2,\n",
    "        \"silent\": 0,\n",
    "        \"objective\": 'multi:softmax',\n",
    "        \"num_round\": 10 \n",
    "    })\n",
    "\n",
    "xgb_estimator.fit({\n",
    "    'train': train_input,\n",
    "    'validation': validation_input\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extract the trained model artefact locally. This could be eventually loaded back into an XGBoost framework Python object and used for re-training or for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {xgb_estimator.output_path}/{xgb_estimator.latest_training_job.job_name}/output/model.tar.gz .\n",
    "!tar -xzvf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the objective metric to be validation:merror, which is according to the [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/parameter.html) measured by:\n",
    "\n",
    "`merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "objective_metric_name = \"validation:merror\"\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"alpha\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),\n",
    "    \"lambda\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),\n",
    "    \"eta\": ContinuousParameter(0, 1, scaling_type=\"Linear\"),\n",
    "    \"gamma\": ContinuousParameter(0, 10, scaling_type=\"Linear\")\n",
    "}\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb_estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=9,\n",
    "    max_parallel_jobs=3,\n",
    "    strategy=\"Bayesian\",\n",
    "    objective_type='Minimize'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({\n",
    "    'train': train_input,\n",
    "    'validation': validation_input\n",
    "    },\n",
    "    job_name=\"xgb-randsearch-\" + strftime(\"%Y%m%d-%H-%M-%S\", gmtime()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.HyperparameterTuningJobAnalytics(\n",
    "    tuner.latest_tuning_job.job_name\n",
    ").dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this in the next notebook\n",
    "tuner.latest_tuning_job.job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now move to [Lab3](./3-Deploy.ipynb)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
